{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd362b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 03:19:44.117431: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-02 03:19:44.532955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10477 MB memory:  -> device: 0, name: NVIDIA TITAN V, pci bus id: 0000:a1:00.0, compute capability: 7.0\n",
      "/home/sadman/anaconda3/envs/deeplearning/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import argparse\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from ketos.audio.audio_loader import  AudioFrameLoader, FrameStepper, audio_repres_dict\n",
    "from ketos.neural_networks import load_model_file\n",
    "from ketos.neural_networks.resnet import ResNetInterface\n",
    "from ketos.neural_networks.dev_utils.detection import process, process_audio_loader, save_detections, merge_overlapping_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be8a6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the paths are indicating the location at the PC at DAL!\n",
    "data_dir='/data/WCS/' # Path to the folder that contains all the audio files\n",
    "\n",
    "model='/home/sadman/arctic_mammal/trained_models/bh_detector_v02.kt'\n",
    "output_file_short_note='_bh_detector_v02_' # add anything here to add in the name of the detections file! By default keep it an empty string\n",
    "\n",
    "root_path='/home/sadman/arctic_mammal/' # Path where this program is stored!\n",
    "\n",
    "detection_save_path=root_path+'results/model_detections/'\n",
    "annot_file_path=root_path+'annotations/test/'\n",
    "\n",
    "num_segs=128\n",
    "step_size=None\n",
    "buffer=0.0\n",
    "win_len=1\n",
    "threshold=0.0\n",
    "group=False\n",
    "progress_bar=True\n",
    "merge=False\n",
    "\n",
    "save_detections_flag=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dc8978a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'MagSpectrogram',\n",
       " 'rate': 10000,\n",
       " 'window': 0.051,\n",
       " 'step': 0.01955,\n",
       " 'freq_min': 0,\n",
       " 'freq_max': 5000,\n",
       " 'window_func': 'hamming',\n",
       " 'duration': 3.0,\n",
       " 'normalize_wav': True,\n",
       " 'transforms': [{'name': 'reduce_tonal_noise'},\n",
       "  {'name': 'normalize', 'mean': 0.0, 'std': 1.0}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the classifier and the spectrogram parameters\n",
    "model, audio_repr = load_model_file(model, './tmp_folder', load_audio_repr=True)\n",
    "spec_config = audio_repr[0]['spectrogram']\n",
    "spec_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "613b6c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_detections(detections, save_to):\n",
    "    \"\"\" Save the detections to a csv file\n",
    "\n",
    "        Args:\n",
    "            detections: numpy.array\n",
    "                List of detections\n",
    "            save_to:string\n",
    "                The path to the .csv file where the detections will be saved.\n",
    "                Example: \"/home/user/detections.csv\"\n",
    "    \"\"\"\n",
    "    if len(detections) == 0: return\n",
    "\n",
    "    a = np.array(detections)\n",
    "    df = pd.DataFrame({'filename':a[:,0], 'start':a[:,1], 'duration':a[:,2], 'score':a[:,3]})\n",
    "    include_header = not os.path.exists(save_to)\n",
    "    df.to_csv(save_to, mode='a', index=False, header=include_header)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36878cd",
   "metadata": {},
   "source": [
    "# Annotation file pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6a7ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_annot(annot_df, delete_columns=['Selection', 'View']):\n",
    "    \"\"\" Delete unwanted columns from the annotations dataframe and \n",
    "        apply necessary pre-processing on the annotation columns\n",
    "     \n",
    "        Args:\n",
    "            annot_df: pandas DataFrame\n",
    "                Annotation table.\n",
    "            delete_columns: list\n",
    "                List of columns to delete from the annotations dataframe\n",
    "                default values ('Selection', 'View')\n",
    "\n",
    "        Returns:\n",
    "            annot_df: pandas DataFrame\n",
    "                Annotation table after pre-processing\n",
    "\n",
    "    \"\"\"\n",
    "    # Delete unnecessary columns\n",
    "    for column in delete_columns:\n",
    "        del annot_df[column]\n",
    "        \n",
    "    annot_df.rename({'Begin Path': 'filename',\n",
    "                    'File Offset (s)': 'start',\n",
    "                    'species': 'label'}, axis='columns', inplace =True)\n",
    "\n",
    "    # Modify filepath (Discard the drive location (e.g., D:/))\n",
    "    annot_df['filename']=annot_df['filename'].apply(lambda x: x[3:len(x)]) \n",
    "    # Modify filepath to replace \\ with / in the filename\n",
    "    annot_df['filename']=annot_df['filename'].apply(lambda x: x.replace(\"\\\\\", \"/\")) \n",
    "    # Modify filepath to add the data root dir (/data/WCS/)\n",
    "    annot_df['filename']=annot_df['filename'].apply(lambda x: data_dir+x)\n",
    "    # Calculate End time\n",
    "    annot_df['end']=annot_df['start']+annot_df['Delta Time (s)']\n",
    "    return annot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b74f7b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name CB50_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/728 [00:00<?, ?it/s]2021-12-02 03:22:10.857909: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-12-02 03:22:11.665566: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8201\n",
      "100%|█████████████████████████████████████████| 728/728 [24:59<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93200 detections saved to /home/sadman/arctic_mammal/results/model_detections/detections_bh_detector_v02_CB50_test.csv\n",
      "file_name CB300_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 410/410 [13:59<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52600 detections saved to /home/sadman/arctic_mammal/results/model_detections/detections_bh_detector_v02_CB300_test.csv\n"
     ]
    }
   ],
   "source": [
    "# load each test file and do all the calculations on them\n",
    "for file_path in glob.glob(annot_file_path+'*.csv'):\n",
    "    annot_df = pd.read_csv(file_path)\n",
    "    annot_df=preprocess_annot(annot_df)\n",
    "    file_list = list(annot_df['filename'].unique())\n",
    "    \n",
    "    # extract the audio folder path\n",
    "    last_index_of_slash=file_list[0].rindex('/')\n",
    "    audio_folder=file_list[0][0:last_index_of_slash]\n",
    "        \n",
    "    # extract the file name from the folder path in the format of \"filename.csv\"\n",
    "    file_name=file_path[file_path.rindex('/')+1:len(file_path)]\n",
    "    print(\"file_name\", file_name)\n",
    "    \n",
    "    # initialize the audio loader\n",
    "    audio_loader = AudioFrameLoader(frame=spec_config['duration'], \n",
    "                                    step=step_size, \n",
    "                                    path=audio_folder, \n",
    "                                    filename=file_list, \n",
    "                                    repres=spec_config)\n",
    "\n",
    "    # process the audio data\n",
    "    detections = process_audio_loader(audio_loader, \n",
    "                                      model=model, \n",
    "                                      batch_size=num_segs, \n",
    "                                      buffer=buffer, \n",
    "                                      threshold=threshold, \n",
    "                                      group=group, \n",
    "                                      win_len=win_len, \n",
    "                                      progress_bar=progress_bar)\n",
    "    if merge == True:\n",
    "        detections = merge_overlapping_detections(detections)\n",
    "\n",
    "    if save_detections_flag == True:\n",
    "        # get the output file name\n",
    "        if(len(output_file_short_note)==0):\n",
    "            output=detection_save_path+\"detections_\"+file_name\n",
    "        else:\n",
    "            output=detection_save_path+\"detections\"+output_file_short_note+file_name\n",
    "\n",
    "        # save the each detections on test dataset\n",
    "        if os.path.isfile(output): os.remove(output) #remove, if already exists\n",
    "        print(f'{len(detections)} detections saved to {output}')\n",
    "        detections_df=save_detections(detections=detections, save_to=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbd3bca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
